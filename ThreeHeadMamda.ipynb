{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirabbas/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-27 16:29:02.330412: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-27 16:29:02.351800: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-27 16:29:02.351825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-27 16:29:02.352399: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-27 16:29:02.356305: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-27 16:29:02.784392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from utils.loss_function import SaliencyLoss\n",
    "from utils.data_process_uni import TrainDataset,ValDataset\n",
    "from net.models.SUM import SUM\n",
    "from net.configs.config_setting import setting_config\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the dataframe\n",
    "df = pd.read_csv('HandInfo.csv')\n",
    "age_bins = [0, 21, 22, 23, 24, 31, 76]\n",
    "labels = np.arange(6)\n",
    "df['age_category'] = pd.cut(df['age'], bins=age_bins, labels=labels, right=False, include_lowest=True)\n",
    "df = df[df.accessories == 0]\n",
    "df['p'] = np.where(df.aspectOfHand.str.startswith('p') == True, 1, 0)\n",
    "df['r'] = np.where(df.aspectOfHand.str.endswith('right') == True, 1, 0)\n",
    "df_p_r = df[(df.p == 1) & (df.r == 1)]\n",
    "df_p_l = df[(df.p == 1) & (df.r == 0)]\n",
    "df_d_r = df[(df.p == 0) & (df.r == 1)]\n",
    "df_d_l = df[(df.p == 0) & (df.r == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "image_directory = 'Hands'\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "lr = 8e-4\n",
    "split_size = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = setting_config\n",
    "model_cfg = config.model_config\n",
    "# Training and Validation Loop\n",
    "best_loss = float('inf')\n",
    "num_epochs = 10\n",
    "# Early stopping setup\n",
    "early_stop_counter = 0\n",
    "early_stop_threshold = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the data augmentation and normalization transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)], p=0.5),\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "def split_data(df, test_size=split_size):\n",
    "    train_df, val_df = train_test_split(df, test_size=test_size, stratify=df['id'], random_state=42)\n",
    "    return train_df, val_df\n",
    "\n",
    "def encode(col, encoder, val_df, train_df):\n",
    "    encoder.fit(train_df[col])\n",
    "    train_df[col] = encoder.transform(train_df[col])\n",
    "    val_df[col] = encoder.transform(val_df[col])\n",
    "\n",
    "train_df_p_r, val_df_p_r = split_data(df_p_r)\n",
    "train_df_p_l, val_df_p_l = split_data(df_p_l)\n",
    "train_df_d_r, val_df_d_r = split_data(df_d_r)\n",
    "train_df_d_l, val_df_d_l = split_data(df_d_l)\n",
    "\n",
    "train_df = pd.concat([train_df_p_r, train_df_d_r, train_df_p_l, train_df_d_l])\n",
    "val_df = pd.concat([val_df_p_r, val_df_d_r, val_df_p_l, val_df_d_l])\n",
    "data_len = train_df.shape[0]\n",
    "\n",
    "pairs = [('id', LabelEncoder()), ('age_category', LabelEncoder()), ('gender', LabelEncoder())]\n",
    "for pair in pairs:\n",
    "    encode(pair[0], pair[1], val_df, train_df)\n",
    "train_id_one_hot = pd.get_dummies(train_df['id'])\n",
    "val_id_one_hot = pd.get_dummies(val_df['id'])\n",
    "train_age_one_hot = pd.get_dummies(train_df['age_category'])\n",
    "val_age_one_hot = pd.get_dummies(val_df['age_category'])\n",
    "train_gender_one_hot = pd.get_dummies(train_df['gender'])\n",
    "val_gender_one_hot = pd.get_dummies(val_df['gender'])\n",
    "num_classes1 = len(pairs[0][1].classes_)\n",
    "num_classes2 = len(pairs[1][1].classes_)\n",
    "num_classes3 = len(pairs[2][1].classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HandDataset(Dataset):\n",
    "    def __init__(self, dataframe, labels, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.labels1, self.labels2, self.labels3 = labels\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx]['imageName'])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label1 = self.labels1.iloc[idx].values.astype(float)\n",
    "        label2 = self.labels2.iloc[idx].values.astype(float)\n",
    "        label3 = self.labels3.iloc[idx].values.astype(float)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, [torch.tensor(label1), torch.tensor(label2), torch.tensor(label3)]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HandDataset(train_df, [train_id_one_hot, train_age_one_hot, train_gender_one_hot], image_directory, transform=data_transforms['train'])\n",
    "val_dataset = HandDataset(val_df, [val_id_one_hot, val_age_one_hot, val_gender_one_hot], image_directory, transform=data_transforms['val'])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the FullyConnectedNetwork\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, sum_model, num_classes1, num_classes2, num_classes3, input_features, dropout_rate=0.5):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.sum_model = sum_model\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # first fully connected layers\n",
    "        self.fc1_1 = nn.Linear(input_features, 512)\n",
    "        self.bn1_1 = nn.BatchNorm1d(512)\n",
    "        self.fc2_1 = nn.Linear(512, 512)\n",
    "        self.bn2_1 = nn.BatchNorm1d(512)\n",
    "        self.fc3_1 = nn.Linear(512, num_classes1)\n",
    "        # second fully connected layers\n",
    "        self.fc1_2 = nn.Linear(input_features, 256)\n",
    "        self.bn1_2 = nn.BatchNorm1d(256)\n",
    "        self.fc2_2 = nn.Linear(256, 256)\n",
    "        self.bn2_2 = nn.BatchNorm1d(256)\n",
    "        self.fc3_2 = nn.Linear(256, num_classes2)\n",
    "        # third fully connected layers\n",
    "        self.fc1_3 = nn.Linear(input_features, 128)\n",
    "        self.bn1_3 = nn.BatchNorm1d(128)\n",
    "        self.fc2_3 = nn.Linear(128, 128)\n",
    "        self.bn2_3 = nn.BatchNorm1d(128)\n",
    "        self.fc3_3 = nn.Linear(128, num_classes3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sum_model(x)\n",
    "        x = self.flatten(x)\n",
    "        # Forward through the first separate fully connected network\n",
    "        out1 = torch.relu(self.bn1_1(self.fc1_1(x)))\n",
    "        out1 = self.dropout(out1)\n",
    "        out1 = torch.relu(self.bn2_1(self.fc2_1(out1)))\n",
    "        out1 = self.dropout(out1)\n",
    "        out1 = self.fc3_1(out1)\n",
    "        \n",
    "        # Forward through the second separate fully connected network\n",
    "        out2 = torch.relu(self.bn1_2(self.fc1_2(x)))\n",
    "        out2 = self.dropout(out2)\n",
    "        out2 = torch.relu(self.bn2_2(self.fc2_2(out2)))\n",
    "        out2 = self.dropout(out2)\n",
    "        out2 = self.fc3_2(out2)\n",
    "        \n",
    "        # Forward through the third separate fully connected network\n",
    "        out3 = torch.relu(self.bn1_3(self.fc1_3(x)))\n",
    "        out3 = self.dropout(out3)\n",
    "        out3 = torch.relu(self.bn2_3(self.fc2_3(out3)))\n",
    "        out3 = self.dropout(out3)\n",
    "        out3 = self.fc3_3(out3)\n",
    "        \n",
    "        return out1, out2, out3\n",
    "    # Assuming the SUM class is already defined as per your model\n",
    "if config.network == 'sum':\n",
    "    sum_model = SUM(\n",
    "        num_classes=model_cfg['num_classes'],\n",
    "        input_channels=model_cfg['input_channels'],\n",
    "        depths=model_cfg['depths'],\n",
    "        depths_decoder=model_cfg['depths_decoder'],\n",
    "        drop_path_rate=model_cfg['drop_path_rate'],\n",
    "        load_ckpt_path=model_cfg['load_ckpt_path'],\n",
    "    )\n",
    "    sum_model.load_from()\n",
    "    sum_model.cuda()\n",
    "    for param in sum_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define the FullyConnectedNetwork with the SUM model as part of it\n",
    "# Adjust input_features based on the output shape of the SUM model\n",
    "input_features = 37632  # Example value, adjust based on your model's output shape\n",
    "model = FullyConnectedNetwork(sum_model, num_classes1, num_classes2, num_classes3, input_features)\n",
    "model.cuda()\n",
    "\n",
    "# Set up criterion, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Use CrossEntropyLoss with label smoothing\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter('runs/experiment_2')\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=10, base_model_path=\"model.pth\"):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                data_loader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects1 = 0\n",
    "            running_corrects2 = 0\n",
    "            running_corrects3 = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for batch_idx, (inputs, labels) in enumerate(data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels1, labels2, labels3 = labels\n",
    "                labels1 = labels1.to(device)\n",
    "                labels2 = labels2.to(device)\n",
    "                labels3 = labels3.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs1, outputs2, outputs3 = model(inputs)\n",
    "                    _, preds1 = torch.max(outputs1, 1)\n",
    "                    _, preds2 = torch.max(outputs2, 1)\n",
    "                    _, preds3 = torch.max(outputs3, 1)\n",
    "                    loss1 = criterion(outputs1, labels1.argmax(dim=1))\n",
    "                    loss2 = criterion(outputs2, labels2.argmax(dim=1))\n",
    "                    loss3 = criterion(outputs3, labels3.argmax(dim=1))\n",
    "                    loss = loss1 + loss2 + loss3\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects1 += torch.sum(preds1 == labels1.argmax(dim=1).data)\n",
    "                running_corrects2 += torch.sum(preds2 == labels2.argmax(dim=1).data)\n",
    "                running_corrects3 += torch.sum(preds3 == labels3.argmax(dim=1).data)\n",
    "\n",
    "                # Print loss status after each batch\n",
    "                if phase == 'train':\n",
    "                    sys.stdout.write(f'\\rBatch {batch_idx}/{len(data_loader) - 1} Loss: {loss.item():.4f}')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc1 = running_corrects1.double() / len(data_loader.dataset)\n",
    "            epoch_acc2 = running_corrects2.double() / len(data_loader.dataset)\n",
    "            epoch_acc3 = running_corrects3.double() / len(data_loader.dataset)\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar(f'{phase}/Loss', epoch_loss, epoch)\n",
    "            writer.add_scalar(f'{phase}/Accuracy1', epoch_acc1, epoch)\n",
    "            writer.add_scalar(f'{phase}/Accuracy2', epoch_acc2, epoch)\n",
    "            writer.add_scalar(f'{phase}/Accuracy3', epoch_acc3, epoch)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc1: {epoch_acc1:.4f} Acc2: {epoch_acc2:.4f} Acc3: {epoch_acc3:.4f}')\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "                avg_acc = (epoch_acc1 + epoch_acc2 + epoch_acc3) / 3\n",
    "                if avg_acc > best_acc:\n",
    "                    best_acc = avg_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                    torch.save(model.state_dict(), base_model_path)  # Save the base model\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch}')\n",
    "                    model.load_state_dict(best_model_wts)\n",
    "                    writer.close()\n",
    "                    return model\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Batch 196/196 Loss: 5.5074train Loss: 6.5133 Acc1: 0.0933 Acc2: 0.4839 Acc3: 0.8253\n",
      "val Loss: 4.8811 Acc1: 0.2840 Acc2: 0.6398 Acc3: 0.9428\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Batch 3/196 Loss: 5.8252"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 122\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, base_model_path)\u001b[0m\n\u001b[1;32m    120\u001b[0m _, preds2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs2, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    121\u001b[0m _, preds3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs3, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m criterion(outputs2, labels2\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    124\u001b[0m loss3 \u001b[38;5;241m=\u001b[39m criterion(outputs3, labels3\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Train the model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.5065train Loss: 1.9949 Acc: 0.7058\n",
      "val Loss: 1.2943 Acc: 0.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/video_mamba/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.6103train Loss: 1.4719 Acc: 0.8856\n",
      "val Loss: 1.1393 Acc: 0.9835\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.2699train Loss: 1.3123 Acc: 0.9429\n",
      "val Loss: 1.0931 Acc: 0.9917\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.3738train Loss: 1.2305 Acc: 0.9673\n",
      "val Loss: 1.0656 Acc: 0.9911\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.1671train Loss: 1.1881 Acc: 0.9722\n",
      "val Loss: 1.0386 Acc: 0.9956\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "Batch 150/196 Loss: 1.1899"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "for name, param in sum_model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "num_epochs = 15\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.95, total_iters=num_epochs)\n",
    "# Fine-tune the entire model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs, patience=10, base_model_path = 'besti.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./net/pre_trained_weights/new_best.pth\", map_location=\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
