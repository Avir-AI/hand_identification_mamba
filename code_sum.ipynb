{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/video_mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from utils.loss_function import SaliencyLoss\n",
    "from utils.data_process_uni import TrainDataset,ValDataset\n",
    "from net.models.SUM import SUM\n",
    "from net.configs.config_setting import setting_config\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the dataframe\n",
    "df = pd.read_csv('HandInfo.csv')\n",
    "df = df[df.accessories == 0]\n",
    "df['p'] = np.where(df.aspectOfHand.str.startswith('p') == True, 1, 0)\n",
    "df['r'] = np.where(df.aspectOfHand.str.endswith('right') == True, 1, 0)\n",
    "\n",
    "df_p_r = df[(df.p == 1) & (df.r == 1)]\n",
    "df_p_l = df[(df.p == 1) & (df.r == 0)]\n",
    "df_d_r = df[(df.p == 0) & (df.r == 1)]\n",
    "df_d_l = df[(df.p == 0) & (df.r == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "image_directory = 'Hands'\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "lr = 8e-4\n",
    "split_size = 0.2\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = setting_config\n",
    "model_cfg = config.model_config\n",
    "# Training and Validation Loop\n",
    "best_loss = float('inf')\n",
    "num_epochs = 10\n",
    "# Early stopping setup\n",
    "early_stop_counter = 0\n",
    "early_stop_threshold = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the data augmentation and normalization transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)], p=0.5),\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "def split_data(df, test_size=split_size):\n",
    "    train_df, val_df = train_test_split(df, test_size=test_size, stratify=df['id'], random_state=42)\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df_p_r, val_df_p_r = split_data(df_p_r)\n",
    "train_df_p_l, val_df_p_l = split_data(df_p_l)\n",
    "train_df_d_r, val_df_d_r = split_data(df_d_r)\n",
    "train_df_d_l, val_df_d_l = split_data(df_d_l)\n",
    "\n",
    "train_df = pd.concat([train_df_p_r, train_df_d_r, train_df_p_l, train_df_d_l])\n",
    "val_df = pd.concat([val_df_p_r, val_df_d_r, val_df_p_l, val_df_d_l])\n",
    "data_len = train_df.shape[0]\n",
    "\n",
    "common_ids = set(train_df['id']).intersection(val_df['id'])\n",
    "train_df = train_df[train_df['id'].isin(common_ids)]\n",
    "val_df = val_df[val_df['id'].isin(common_ids)]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['id'])\n",
    "\n",
    "train_df['id'] = label_encoder.transform(train_df['id'])\n",
    "val_df['id'] = label_encoder.transform(val_df['id'])\n",
    "\n",
    "train_labels_one_hot = pd.get_dummies(train_df['id'])\n",
    "val_labels_one_hot = pd.get_dummies(val_df['id'])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HandDataset(Dataset):\n",
    "    def __init__(self, dataframe, labels, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.labels = labels\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx]['imageName'])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.labels.iloc[idx].values.astype(float)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HandDataset(train_df, train_labels_one_hot, image_directory, transform=data_transforms['train'])\n",
    "val_dataset = HandDataset(val_df, val_labels_one_hot, image_directory, transform=data_transforms['val'])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the FullyConnectedNetwork\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, sum_model, num_classes, input_features, dropout_rate=0.5):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.sum_model = sum_model\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_features, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sum_model(x)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the SUM class is already defined as per your model\n",
    "if config.network == 'sum':\n",
    "    sum_model = SUM(\n",
    "        num_classes=model_cfg['num_classes'],\n",
    "        input_channels=model_cfg['input_channels'],\n",
    "        depths=model_cfg['depths'],\n",
    "        depths_decoder=model_cfg['depths_decoder'],\n",
    "        drop_path_rate=model_cfg['drop_path_rate'],\n",
    "        load_ckpt_path=model_cfg['load_ckpt_path'],\n",
    "    )\n",
    "    sum_model.load_from()\n",
    "    sum_model.cuda()\n",
    "    for param in sum_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define the FullyConnectedNetwork with the SUM model as part of it\n",
    "# Adjust input_features based on the output shape of the SUM model\n",
    "input_features = 37632  # Example value, adjust based on your model's output shape\n",
    "num_classes = len(label_encoder.classes_)  # Ensure num_classes is defined\n",
    "model = FullyConnectedNetwork(sum_model, num_classes, input_features)\n",
    "model.cuda(1)\n",
    "\n",
    "# Set up criterion, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Use CrossEntropyLoss with label smoothing\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter('runs/experiment_2')\n",
    "\n",
    "# Training functionâ€\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=10, base_model_path=\"model.pth\"):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                data_loader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for batch_idx, (inputs, labels) in enumerate(data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels.argmax(dim=1))\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.argmax(dim=1).data)\n",
    "\n",
    "                # Print loss status after each batch\n",
    "                if phase == 'train':\n",
    "                    sys.stdout.write(f'\\rBatch {batch_idx}/{len(data_loader) - 1} Loss: {loss.item():.4f}')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader.dataset)\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar(f'{phase}/Loss', epoch_loss, epoch)\n",
    "            writer.add_scalar(f'{phase}/Accuracy', epoch_acc, epoch)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                    torch.save(model.state_dict(), base_model_path)  # Save the base model\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch}')\n",
    "                    model.load_state_dict(best_model_wts)\n",
    "                    writer.close()\n",
    "                    return model\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    writer.close()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Batch 49/49 Loss: 4.5327train Loss: 4.8117 Acc: 0.0672\n",
      "val Loss: 3.9527 Acc: 0.2078\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Batch 49/49 Loss: 3.8745train Loss: 4.0586 Acc: 0.1839\n",
      "val Loss: 3.2025 Acc: 0.4123\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "Batch 49/49 Loss: 3.3416train Loss: 3.4335 Acc: 0.3147\n",
      "val Loss: 2.5879 Acc: 0.5832\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "Batch 49/49 Loss: 2.7555train Loss: 2.9335 Acc: 0.4464\n",
      "val Loss: 2.1131 Acc: 0.7141\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Batch 49/49 Loss: 2.5297train Loss: 2.5896 Acc: 0.5300\n",
      "val Loss: 1.8322 Acc: 0.7891\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "Batch 49/49 Loss: 2.7494train Loss: 2.3023 Acc: 0.6222\n",
      "val Loss: 1.6152 Acc: 0.8647\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "Batch 49/49 Loss: 2.0175train Loss: 2.1247 Acc: 0.6822\n",
      "val Loss: 1.4886 Acc: 0.8952\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "Batch 49/49 Loss: 2.2674train Loss: 1.9810 Acc: 0.7279\n",
      "val Loss: 1.4000 Acc: 0.9288\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "Batch 49/49 Loss: 1.8302train Loss: 1.8821 Acc: 0.7550\n",
      "val Loss: 1.3386 Acc: 0.9377\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "Batch 49/49 Loss: 2.4566train Loss: 1.7999 Acc: 0.7787\n",
      "val Loss: 1.2999 Acc: 0.9460\n",
      "\n",
      "Best val Acc: 0.9460\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Train the model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.5065train Loss: 1.9949 Acc: 0.7058\n",
      "val Loss: 1.2943 Acc: 0.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/video_mamba/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.6103train Loss: 1.4719 Acc: 0.8856\n",
      "val Loss: 1.1393 Acc: 0.9835\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.2699train Loss: 1.3123 Acc: 0.9429\n",
      "val Loss: 1.0931 Acc: 0.9917\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.3738train Loss: 1.2305 Acc: 0.9673\n",
      "val Loss: 1.0656 Acc: 0.9911\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "Batch 196/196 Loss: 1.1671train Loss: 1.1881 Acc: 0.9722\n",
      "val Loss: 1.0386 Acc: 0.9956\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "Batch 150/196 Loss: 1.1899"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "for name, param in sum_model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "num_epochs = 15\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.95, total_iters=num_epochs)\n",
    "# Fine-tune the entire model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs, patience=10, base_model_path = 'besti.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"new_best.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_mamba",
   "language": "python",
   "name": "video_mamba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
